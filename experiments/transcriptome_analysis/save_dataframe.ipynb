{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "319e8e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "cwd = Path(os.getcwd())\n",
    "WORKING_DIR = cwd.parent\n",
    "DATASET_DIR = WORKING_DIR.parent / \"dataset\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16cb32eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class prob0 accounts for 99.66% of the predictions\n",
      "Class prob1 accounts for 0.08% of the predictions\n",
      "Class prob2 accounts for 0.16% of the predictions\n",
      "Class prob3 accounts for 0.02% of the predictions\n",
      "Class prob4 accounts for 0.09% of the predictions\n"
     ]
    }
   ],
   "source": [
    "res = cwd / (f\"transcriptome/predictions_hard_mining.parquet\")\n",
    "preds = pd.read_parquet(res)\n",
    "preds[\"max_type\"] = preds[[\"prob0\",\"prob1\", \"prob2\", \"prob3\", \"prob4\"]].idxmax(axis=1)\n",
    "\n",
    "counts_all = np.unique(preds[\"max_type\"], return_counts=True)\n",
    "fractions = counts_all[1] / preds.shape[0] * 100\n",
    "for class_, count in zip(counts_all[0], fractions):\n",
    "    print(f\"Class {class_} accounts for {count:.2f}% of the predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e516da20",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_file = cwd / \"transcriptome/gencode_customized.tsv\"\n",
    "tx = pd.read_csv(tx_file, sep=\"\\t\")\n",
    "tx[\"ids\"] = tx.index\n",
    "mapping_to_genes = tx[\"gene_id\"].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e90b9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading existing unusable sequences from /home/saitto/rna_modifications_project/experiments/transcriptome_analysis/transcriptome/unusable_sequences.pickle\n"
     ]
    }
   ],
   "source": [
    "#we generate or load the unusable positions  in the transcriptome, which correspond to sequences long 51 already present in the training dataset\n",
    "unusable_path = cwd / \"transcriptome/unusable_sequences.pickle\"\n",
    "\n",
    "if unusable_path.exists():\n",
    "    print(f\"Loading existing unusable sequences from {unusable_path}\")\n",
    "    with open(unusable_path, \"rb\") as f:\n",
    "        unusable_sequences = pickle.load(f)\n",
    "else:\n",
    "    print(f\"File not found. Generating unusable sequences...\")\n",
    "    \n",
    "    # Load the dataset and process sequences\n",
    "    path_full_dataset = DATASET_DIR / \"full_dataset_final_training.pickle\"\n",
    "    with open(path_full_dataset, \"rb\") as f:\n",
    "        full_dataset = pickle.load(f)\n",
    "    \n",
    "    resized_sequences = []\n",
    "    for key, value in full_dataset.items():\n",
    "        for i in range(len(value)):\n",
    "            seq = value[i]\n",
    "            seq = seq[50:151-50]\n",
    "            resized_sequences.append(seq)\n",
    "\n",
    "    resized_sequences = set(resized_sequences)\n",
    "\n",
    "    # Find unusable sequences\n",
    "    unusable_sequences = defaultdict(list)\n",
    "    for index, row in tx.iterrows():\n",
    "        seq = row[\"sequence\"]\n",
    "        c_pos = np.where(np.array([x for x in seq]) == \"C\")[0]\n",
    "        for pos in c_pos:\n",
    "            cut_seq = seq[pos-25:pos+26]\n",
    "            if cut_seq in resized_sequences:\n",
    "                unusable_sequences[index].append(int(pos))\n",
    "    unusable_sequences = dict(unusable_sequences)\n",
    "\n",
    "    with open(unusable_path, \"wb\") as f:\n",
    "        pickle.dump(unusable_sequences, f)\n",
    "    \n",
    "    print(f\"Unusable sequences generated and saved to {unusable_path}\")\n",
    "\n",
    "unrolled_unusable_sequences = set()\n",
    "for key, value in unusable_sequences.items():\n",
    "    for pos in value:\n",
    "        unrolled_unusable_sequences.add((key, pos))\n",
    "\n",
    "# Add an 'is_unusable' column that's True when the pair exists in the set\n",
    "# Create a list of tuples from the DataFrame columns\n",
    "tx_pos_pairs = list(zip(preds['tx_idx'], preds['center_pos']))\n",
    "\n",
    "# Check if each pair is in the set (faster than apply for large DataFrames)\n",
    "preds['is_unusable'] = [pair in unrolled_unusable_sequences for pair in tx_pos_pairs]\n",
    "preds = preds.loc[preds[\"max_type\"] != \"prob0\"]\n",
    "preds[\"prob_max\"] = preds[[\"prob1\", \"prob2\", \"prob3\", \"prob4\"]].max(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef6de0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to save final\n",
    "nice_names   = dict(prob0=\"Negatives\", prob1=\"I\", prob2=\"II\",\n",
    "                    prob3=\"III\", prob4=\"IV\")\n",
    "\n",
    "preds_to_save = preds.copy()\n",
    "preds_to_save.drop(columns=[\"prob0\", \"prob1\", \"prob2\", \"prob3\", \"prob4\"], inplace=True)\n",
    "tx_to_keep = tx[tx[\"ids\"].isin(preds_to_save[\"tx_idx\"].tolist())]\n",
    "tx_to_keep = tx_to_keep.merge(preds_to_save, left_on=\"ids\", right_on=\"tx_idx\", how=\"inner\")\n",
    "tx_to_keep.drop(columns=[\"ids\", \"tx_idx\"], inplace=True)\n",
    "tx_to_keep[\"sequence\"] = tx_to_keep[\"sequence\"].apply(lambda x: x[25:])\n",
    "tx_to_keep[\"center_pos\"] = tx_to_keep[\"center_pos\"].apply(lambda x: x - 25)\n",
    "tx_to_keep[\"max_type\"] = tx_to_keep[\"max_type\"].map(nice_names)\n",
    "tx_to_keep.rename(columns={\"max_type\": \"Type\", \"prob_max\": \"probability\", \"is_unusable\": \"in_train_or_test_sets\", \"center_pos\": \"position\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23772e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "tx_to_keep_no_seq = tx_to_keep.copy()\n",
    "tx_to_keep_no_seq.drop(columns=[\"sequence\"], inplace=True)\n",
    "\n",
    "tx_to_keep_no_seq.to_csv(cwd / \"analysis_results/m5C_predictions.tsv.gz\",\n",
    "               sep=\"\\t\", index=False, compression=\"gzip\")\n",
    "\n",
    "tx_to_keep_no_seq.to_csv(cwd / \"analysis_results/m5C_predictions.tsv\",\n",
    "               sep=\"\\t\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "363688e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tx_to_keep_no_seq.to_excel(cwd / \"analysis_results/m5C_predictions.xlsx\", index=False)\n",
    "\n",
    "# #save to keep with sequence in normal tsv format\n",
    "# tx_to_keep.to_csv(cwd / \"analysis_results/m5C_predictions_with_seq.tsv\",\n",
    "#                sep=\"\\t\", index=False, compression=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f7f04e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hamming_distance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
